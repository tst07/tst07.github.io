var tipuesearch = {"pages":[{"title":"About Me","text":"Hi! My name is Prakhar and welcome to my website. While choosing Software Engineering as my professional career, I am also interested in music (both listening and playing), cinematography, writing and storytelling. I have huge fascination for art, whether it is drawing a cartoon on paper with a ball pen or a ten year long anime, I am all for it. If you are an artist who plays music/likes drawing/do comedy or involve in any creative endeavour, I am already a fan of you and will always root for you! To have light hearted time, I usually watch a podcast/watch some comic videos or sometimes watch a movie or tv series recommended by my friends. Even better if I have some friends with me and couple of beers. As an engineer, I have built applications from scratch which have been catered to PAN India and they are currently helping the law enforcement agencies to maintain law and order in our beloved country. I love writing code in python, whether it be flask/django/pyspark or anything related. I like to imagine myself building AI which will work for the welfare of humankind. While it is mostly daydreaming, There is certain truth to it and if given an opportunity, I am willing to make it happen at all cost. Find my resume here: Prakhar Mishra","tags":"pages","url":"https://prakharmishra.com/pages/About.html","loc":"https://prakharmishra.com/pages/About.html"},{"title":"Contact Me","text":"Thank you for taking time to visit my internet home! If you want to connect further: Send an invite on LinkedIn: LinkedIn Send DM on Instagram: Instagram Drop me a mail at: prakharmishra2593@gmail.com","tags":"pages","url":"https://prakharmishra.com/pages/Contact.html","loc":"https://prakharmishra.com/pages/Contact.html"},{"title":"I Wrote My First Song!!","text":"I am learning guitar for past 3 years now. Despite being able to cover the soulful and touching music from my favourite artists, I was never satisfied with just looking at tabs and covering the existing songs. I always wanted to create my own songs and give music to them with the help of guitar. Songwriting is hard. Whenever I tend to write something, I always fear that it will turn out to be cringy. Well if that is not enough, singing is hard too xd. Let me tell you honestly, I am super insecure about my singing and even when I was singing for this, I made sure that my voice can't leave my room and be heard by no living soul :D. Numerous times I have started writing and then given up because I thought either it sounded cringe or it was very bland and had no soul. I have gone a bit cold inside that I do not feel lot of things very emotionally, but that night I was feeling a bit off with certain things and it hopefully reflected in the song. I was also somewhat satisfied with how it ended up sounding and also with the guitar part. The lesson learned is, I can't write whenever I want to. I have to feel strongly about something only then I can perhaps reflect that in my writing. I will also need to find my voice, I have watched some tutorials on youtube and each one has the message that anyone can sing. It takes time and practice to find your own voice. Whether they are right or just selling positive vibes, I am willing to try out. One more thing, the song turned out way different than it sounded in my head. Why was that? I don't know the answer. Maybe the great artists are the ones who are able to close this gap by practicing the skill or being innately good with the art. Whatever the case be, I would say that it gives a bit of satisfaction when you take the first step and also knowing you can only get better from here. I don't have any knowledge of DAW (digial audio workstation) software, I somehow managed to figure out how to record with simple record, play and pause buttons in ableton. I think the next steps are learning various tools of DAW so I can fine tune the audio which will make it sound even better! If I continue writing new songs and keep learning DAW, it will be so nice to see the difference one day from where I started. With the hopes of great future I am ending this blog. You can find below the link and listen to the song. As always, Thanks for taking time to read the article, I will see you in the next one! Please find the video demonstration youtube video link: DISCLAIMER: The author doesn't intend to be expert on the problems. While these notes will solve your problem at hand, it is always advised to dig deep to the problem if one is interested and also share your further learnings in the comments below as well.","tags":"music","url":"https://prakharmishra.com/articles/my_first_song.html","loc":"https://prakharmishra.com/articles/my_first_song.html"},{"title":"Pyspark: Basics And Computation Of Cluster Strength for Spark Jobs","text":"When I started on working with pyspark to explore big data horizon, I was told by the experienced engineers the dos and donts of running a spark project. I was supposed to follow their rules until I get good enough and understand the underlying intrinsic details of spark so that I don't fuck up the project and give my teammates some sleepless nights :) Luckily it didn't happen and now that I have gathered the knowledge, Its time to give back to my fellow engineers who are starting their big data journey. When starting out, my collegue told me that whenever you write an instruction, whether a transformation/collection or a loading functions in your code, always ask yourself if this will be able to run on a single machine itself. This will help you keep away from traditional programming logics and everything else would follow with googling and figuring out how to do the task in distributed manner. In this article, Let's discuss some spark terminologies and figure out what cluster requirements are for runnning our spark jobs efficiently. Cluster: Cluster is nothing but a set of computers dedicated to run our spark job on huge data. It can comprise of multiple servers, each server can be multicore and with some dedicated memory to it. Let's say we have a cluster of 10 nodes, each node has 16 cores and 64GB RAM attached to it. RDD: RDD stands for resilient distributed dataset, which is more often called the fundamental data structure of spark. Think of RDD as a large data which will be first loaded and then divided into partitions by spark to achieve parallel execution on these partitions. RDD can be loaded to spark from HDFS, Database or from a huge file sitting in a disk. DataFrame: DataFrame is API written over RDD, which is used when you have structured data (data for which you can imagine a schema). While stored data can be very chaotic, if you happend to have a schema based dataset, it is highly recommended to not use RDD and use DataFrame API since it already has many optimizations out of the box. Partition: The smallest unit of large data which goes to the cluster and gets modified or transformed by spark job! Suppose we have a 200G file which is loaded in our rdd, spark is going to divide the RDD into chunks of 128MB size partitions and perform the operations parallely on these chunks by sending them to various nodes in our cluster. Ideally partition size should not exceed more than 128MB. So for a 200G file, number of partitions will be 200*1024/128 = ~1600 Task: Task is a unit of work which will be done on a partition. One can have as many tasks as number of partitions, which will enable parallel execution of all partitions in same 'stage' (more on stage later). If you have more tasks available than partitions, then some will be idle. If you have less tasks than partitions then some partitions will have to wait for the tasks to finish their existing work and then pick them up. Executor: An executor is a process which combined with other identical executers run on a single server. Yes, that is correct, a single machine in a cluster can have multiple executors. Each executor can have capacity of running multiple tasks. Number of tasks and number of executor in a machine depend on the machine's CPU cores. For best practices, an executor is not assigned more than 5 cores. So suppose you have 32 core machine, then you can run 32/5 = 6 executors in that machine. Assigning more cores to an executor is shown to slow down the jobs significantly. So remember this, 5 or less cores per executor!!! Which will correlate to running 5 or less tasks per executor. We will further read about the 'shuffle block' and HDFS also performs best with 5 task per executor. For an example, we have cluster of 10 machines, each have 32 Core CPU and 64 GB memory. Let's derive how many executors and tasks we will be able to run parallel. As we discussed earlier, number of executors per machine is 32/5 = ~6. How much memory each executor will have? Answer is 64G/6 = ~10 GB. So, how many total executors will be available across cluser? no_of_executor_in_single_machine * no_of_machines = 6 * 10 = 60 How many tasks we will have at hand at a time? no_of_executors*no_of_cores_per_executor = 60 * 5 = 300 tasks! One more thing that we need to be careful about is that our executors should not transfer more than 2G data to each other. Otherwise we will see 'size limit exceed' error while running our jobs. This transfer of data between executors across stages is called Shuffle Block . Normally when you are using 5 cores per executor and partitions are 128MB, this problem won't arise. Worker: Each node in cluster will have single worker. This worker will manage all the executors running in that node and contact with cluster manager about the availability of executors. Each application or job run will have it's own set of executors. One can think of running more than one worker instance in a node, but it simply isn't effective in improving efficiency since it is merly used for tracking executors and contacting cluster manager. Cluster Manager: Spark can use various cluster manager such as yarn, mesos or standalone. In Spark's standalone CM, you can see job statistics, information about executors, memory consumption, concurrent running tasks etc. Worker instance has the responsibility of communication with cluster manager about the availability of it's resources. Once an executor is assigned to the job, it can directly communicate with driver job without interfering with worker or cluster manager. How spark runs a job: You can tell spark that it's lazy without hurting it's feelings. Spark delays execution of tasks as long as it can. All data transformation steps which we are writing are not happening until we explicitly ask to collect our output or the next stage of job depend on the results of previous stage. Behind the scenes spark creates a directed acyclic graph containing all the operations, then proceeds to run that in stages. It optimizes the number of as many tasks which can run parallel in one stage, and then pass the output of the stage to next and proceeds to run next stage similarly. In more stepwise manner, this is what happens when we run our job: Spark builds a DAG according to our job. Split the graph into stages of tasks. Run first stage tasks with help of cluster manager to assign executors collect back results from executors as output or proceed to next stage with results. And so on.. While complete workflow can get much more complex including retrying failed tasks, scheduling tasks. This is the most simplistic way on how spark workflow happens behind the scenes. I hope you enjoyed this blog and got basic idea about the workings of spark. I will see you in the next one!! DISCLAIMER: The author doesn't intend to be expert on the problems. While these notes will solve your problem at hand, it is always advised to dig deep to the problem if one is interested and also share your further learnings in the comments below as well.","tags":"programming","url":"https://prakharmishra.com/articles/pyspark_basics_and_computing_cluster_requirement_for_jobs.html","loc":"https://prakharmishra.com/articles/pyspark_basics_and_computing_cluster_requirement_for_jobs.html"},{"title":"Python - When To Use Asyncio, Multithreading, Multiprocessing","text":"Okay, even though we are restricted by the GIL (global interpreter lock) in python, we are still well equipped to face and solve any form of parallel or concurrent programming in python. Lot of people just read about the GIL restrictions and then go \"ohh this is so bad.. how python can be so popular even with a restriction like this?\". But the truth is far from it, while in other languages multithreading is heart and soul to achieve parallelization, in python it is just a module to solve a particular type of use case. We are also provided other modules such as asyncio, multiprocessing, concurrent.futures and even celery for achieving concurrency and parallelization beyond a single machine. So which module is the best fit for our needs? Let's read ahead and find out! Multithreading: This is the most favourite thing about a language for programmers and whenever they switch to a language they will try to figure out how multithreading is implemented in that language. In python, the multithreading is controlled by GIL, which only allows one thread to run at a time. since python is built over C language, lot of libraries which had to be integrated from C to Python required thread safe memory management otherwise the results would be inconsistent. So that's when it was decided to use GIL, it's one single lock to provide thread safe memory environment. Now, does it make multithreading completely useless in python? No. What if we don't even have to run parallel threads in our program. Suppose our program needs IO operations in each thread, Do we really care then if the cpu is taken away from that thread while the IO bound operation completes? CPU can be utilized by some other thread to complete its CPU bound stuff. So in conclusion, if our problem statement is a mixture of CPU bound and IO bound tasks, python's multithreading can easily fit to solve the problem! Multithreading will help us achieve concurrency but not parallelization. But before you go on and implement the multithreading in your problem statement, we have to talk about asyncio and how it came to existence. Asyncio: Consider you have implemented multithreading in your program, and then you realize that you don't actually need concurrent code execution, you just want to manage IO bound tasks effectively. The choke point of your program is slow IO bound operations and running multiple threads is doing very little to none in helping achieving the efficiency. Moreover, you also want to have control over how and when the IO bound task are run since in multithreading, the cpu switching control lies with the library. All that power is provided to you by Asyncio. Consider using asyncio when your program has very slow IO bound operations. For ex. if you have to fetch data from database where queries run for a long time, Asyncio is the best to adapt. Since there will be multiple connections needed to run multiple long running queries, Asyncio can fit to the problem easily. Multiprocessing: Finding if multiprocessing is good fit for the problem is easy. Very little to no IO bound operations such as disk write, db queries, printer etc and more and more CPU powered instructions such as solving mathematical problems, doing calculations, computing digits of pi etc. If your program is CPU intrinsic, having more CPU or faster CPU will always make it faster. Celery: Celery is your friendly neighbourhood background worker. In celery, you can specify the number of workers (read process) and also concurrency level for each worker (which run as threads). Celery is decoupled with your app server, so it provides a much needed independent handling of both background tasks and app server. It is often used with web application development alongside frameworks such as Flask/Django. while managing state between multiple workers is not possible, but one can solve it creatively using cache such as redis (preferred) or saving state to DB as well. Celery also allows you to go beyond a single server architecture, making use of multiple server to run your worker. Combine it with a massage queue such as RabbitMQ and consider all your problems found a solution. You can run particular worker in a single server or in multiple servers, or you can also gather data returned by different workers across multiple servers to one specific server, all of these things are possible through creative and careful configuration of celery. If that was not enough, celery also provides scheduling and periodic tasks as well. Say no to cron jobs which restrict you to a single server or managing crons on different servers so they don't give you inconsistent data hence nightmares. While saying all the good things about celery is not possible in this short writeup, I think I have done the job to tell you how powerful celery can be in your python applications. So go on and give it a try and prepare to be blown away by the awesomeness! Final Note: So, I have listed (all?) the scenarios and conditions which I hope will help you in making decision to find the right module. A good project not just uses one of these libraries, but all of them depending upon the various kind of problems in the project. So mix them up, be careful that you are not under utilizing or overkilling with using one of them. And be a better programmer. Also If you are going to the path of multithreading and multiprocessing, I recommend you to checkout the python3's concurrent.future module. It will make your code much short and easily understandable. That's all for it, I will see you in the next one! DISCLAIMER: The author doesn't intend to be expert on the problems. While these notes will solve your problem at hand, it is always advised to dig deep to the problem if one is interested and also share your further learnings in the comments below as well.","tags":"programming","url":"https://prakharmishra.com/articles/python-multithreading-multiproceessing-or-asyncio.html","loc":"https://prakharmishra.com/articles/python-multithreading-multiproceessing-or-asyncio.html"},{"title":"Python - Call By Value or Reference?","text":"Programmers who start with C++ and then learn python often get confused with how python actually passes the variables to the functions. In C++, there is a clear distinction between call by value and call by reference which uses pointers, But in python there is no concept of a pointer. This also is a very popular question in python programming interviews, combined with the various other questions which often sound like \"Will this variable be updated after this operation?\". Before we discuss the nature of python, we first have to identify how python actually stores the data behind the scenes. Python contains two types of data structurs- Mutable (list, dictionary, sets etc.) and Immutable (numbers, strings , tuples etc). Mutable objects can be modified, Immutable objects are non-modificable. Now, whenever you create a variable and initialize a data structure to it, what python actually doing is creating an object of that data structure and giving it a name which you picked as your variable name. In a way, the variables in python are just names and nothing more than that! So an object can have multiple names, but what matters is all those name will point to the same object. Much like in real life, where a person can be called by multiple names but the person remains the same. Let's see an example: >>> x = 1 # Integer Object is assigned a name x >>> y = x >>> z = 1 >>> >>> print ( id ( x ), id ( y ), id ( z )) # all have same reference 4373842224 4373842224 4373842224 >>> >>> li1 = [ 1 , 2 , 3 ] >>> li2 = li1 >>> li3 = [ 1 , 2 , 3 ] >>> >>> print ( id ( li1 ), id ( li2 ), id ( li3 )) # li3 has a different object reference 4384778176 4384778176 4379968640 So what we end up realizing is, python is internally optimized for memory. It will create a new object when it really feels the need to create one. Otherwise it will keep reusing and same object will be identified by multiple names. Now lets check what the mainstream advice is in this matter: If object is immutable then the modified value is not available outside the function. If object is mutable then modified value is available outside the function. No. Nope.. Not at all. Let's check the example Below: >>> def add_value_to_list ( li ): ... li . append ( 10 ) ... >>> x = [ 1 , 2 , 3 ] >>> add_value_to_list ( x ) # All well and good, since list is mutable, 10 will be appended to it and value will update outside function as well. >>> x [ 1 , 2 , 3 , 10 ] But now let's look at a contradictory example where >>> def change_to_dict ( li ): ... li = { 'a' : 1 } ... print ( id ( li )) ... >>> x = [ 1 , 2 , 3 , 4 ] >>> id ( x ) 4373501888 >>> change_to_dict ( x ) # new reference value is assigned to li inside funtion. But outside, the reference and value remained the same as before. 4363534400 >>> x [ 1 , 2 , 3 , 4 ] >>> id ( x ) 4373501888 What the frick?? Python.. why have you forsaken me here? Well as much as we wanted to fit the two rules to find answers for all the questions, turns out they are not enough. Perhaps a much better answer to the question is, python decides internally when it is sufficient to use same reference object and carry out modifications (called as pass by reference) OR should it create a new one entirely (called as pass by value). In that sense, python is neither a call by value or a call by reference language. I hope you liked this article. I will see you in the next one! DISCLAIMER: The author doesn't intend to be expert on the problems. While these notes will solve your problem at hand, it is always advised to dig deep to the problem if one is interested and also share your further learnings in the comments below as well.","tags":"programming","url":"https://prakharmishra.com/articles/python-call-by-value-or-reference.html","loc":"https://prakharmishra.com/articles/python-call-by-value-or-reference.html"},{"title":"Design Patterns - Composite Pattern Implementation Using C++","text":"We have already covered composite pattern implementation using python in a separate blog. In this blog, let's checkout how we can code using c++. We are implementing something similar to the linux file tree. Now let's get to the implementation. First we will create filesystem.h file where our pattern implementation will live: #include <iostream> #include <list> using namespace std ; class IFileSystem { protected : string name ; public : IFileSystem ( string fname ) { name = fname ; } virtual void display () = 0 ; virtual void operation () = 0 ; virtual void addChild ( IFileSystem * fs ) { // } virtual void removeChild ( IFileSystem * fs ) { // } }; class Dir : public IFileSystem { protected : list < IFileSystem *> children ; public : Dir ( string fname ) : IFileSystem ( fname ) { // } void display () { cout << name << endl ; } void operation () { this -> display (); for ( list < IFileSystem *>:: iterator it = children . begin (); it != children . end (); it ++ ) { ( * it ) -> operation (); } } void addChild ( IFileSystem * fs ) { children . push_back ( fs ); } void removeChild ( IFileSystem * fs ) { list < IFileSystem *>:: iterator it = std :: find ( children . begin (), children . end (), fs ); children . erase ( it ); } }; class LeafFile : public IFileSystem { public : LeafFile ( string fname ) : IFileSystem ( fname ) { // } void display () { cout << name << endl ; } void operation () { this -> display (); } }; Now, Let's create a main.cpp file to make use of our design pattern: #include <iostream> #include \"filesystem.h\" using namespace std ; int main () { IFileSystem * root = new Dir ( \"/\" ); IFileSystem * home = new Dir ( \"/home\" ); IFileSystem * leafFile = new LeafFile ( \"myfile\" ); home -> addChild ( leafFile ); IFileSystem * etc = new Dir ( \"/etc\" ); IFileSystem * var = new Dir ( \"/var\" ); IFileSystem * lib = new Dir ( \"/lib\" ); root -> addChild ( home ); root -> addChild ( etc ); root -> addChild ( var ); root -> addChild ( lib ); root -> operation (); return 0 ; } This was my implementation of linux file tree using c++, Try to run this on your machine and do tweaks and extensions as per your imagination. I will see you in the next one! Related Links Design Patterns - Composite Pattern Implementation Using Python DISCLAIMER: The author doesn't intend to be expert on the problems. While these notes will solve your problem at hand, it is always advised to dig deep to the problem if one is interested and also share your further learnings in the comments below as well.","tags":"programming","url":"https://prakharmishra.com/articles/composite-pattern-using-cpp.html","loc":"https://prakharmishra.com/articles/composite-pattern-using-cpp.html"},{"title":"Design Patterns - Composite Pattern Implementation Using Python","text":"Composite pattern is a structural design patt...... Pfft! I am no good with definitions and most of the time I wouldn't even remember this implementation has even a name. But having said this, software design patterns are one of the most awesome things there in software development that show true power of object oriented programming and provide 'neat' solution to the coding challenges. While writing code, the probability to be working on an original problem is very slim, It's almost certain that some other programmer was also faced with the same problem in history and they had to write the solution for that. Over time many programmers found these repeated problems and decided to generalise the solution and coding style which we now call as the great DESIGN PATTERNS! These are the language which all programmers understand to quickly catch up with each other in terms of code. Programmer1: \"We have implemented this using decorator pattern.\" New hired programmer: \"Ohh I know that! let me check out the code and see how the use case is molded to fit in the pattern!\" See? Easy. After you learn a pattern, try to internalise it with thinking various use cases where it can be useful. And the first time when you see a problem where the pattern will fit and you start coding will be a happiest gotcha moment! One of my favourite design pattern is composite pattern. I have used it so many times now that finding the application of it in a problem has become so natural. I remember when the first time I read about it, The first application that came to my mind was File System of an operation system. How in Unix based system everything starts with a '/' and continue to grow into directories and files from there. Each directory can contain files or more directories inside. So in that sense, files are leaf nodes of file tree and directories are intermediate nodes which can contain leaf nodes or more directory nodes. Perfect for composite pattern! Now let's get to the implementation. We will create composite.py file where our pattern implementation will live: from abc import ABCMeta , abstractmethod class IFileSystem ( metaclass = ABCMeta ): _name = None @abstractmethod def operation ( self , * args , ** kwargs ): pass @abstractmethod def display ( self ): pass class Dir ( IFileSystem ): # _children = [] # _name = None def __init__ ( self , dir_name , * args , ** kwargs ): self . _name = dir_name self . _children = [] def operation ( self ): self . display () for obj in self . _children : obj . operation () def display ( self ): print ( self . _name ) def addChild ( self , child , * args , ** kwargs ): self . _children . append ( child ) def removeChild ( self , child , * args , ** kwargs ): try : ind = self . _children . index ( child ) except : raise Exception ( 'No such item present' ) self . _children . pop ( ind ) class LeafFile ( IFileSystem ): # _name = None def __init__ ( self , file_name , * args , ** kwargs ): self . _name = file_name def display ( self ): print ( self . _name ) def operation ( self ): self . display () Now, Let's create a main.py file to make use of our design pattern: from composite import Dir , LeafFile if __name__ == '__main__' : root = Dir ( \"/\" ) opt = Dir ( \"/opt\" ) opt . addChild ( LeafFile ( \"myfile\" )) root . addChild ( opt ) root . addChild ( Dir ( \"/data\" )) root . addChild ( Dir ( \"/var\" )) root . addChild ( Dir ( \"/etc\" )) root . addChild ( Dir ( \"/home\" )) opt . operation () This was my implementation of linux file tree using python, Try to run this on your machine and do tweaks and extensions as per your imagination. I will see you in the next one! Related Links Design Patterns - Composite Pattern Implementation Using C++ DISCLAIMER: The author doesn't intend to be expert on the problems. While these notes will solve your problem at hand, it is always advised to dig deep to the problem if one is interested and also share your further learnings in the comments below as well.","tags":"programming","url":"https://prakharmishra.com/articles/composite-pattern-using-python.html","loc":"https://prakharmishra.com/articles/composite-pattern-using-python.html"},{"title":"Encrypt Sensitive Data Using AES before sending to server over Http/s (Javascript, Python)","text":"Often times software developers are found creating user authentication for their website. Most of the times they build the complete end to end systems which includes strong password policies, SHA-1 encryptes passwords which are saved in database in 'User' table, User account lock after multiple unsuccessful login attempts and yada yada, but they miss out on encrypting passwords before sending to server. When we see the websites of big tech companies, there is almost never a case where a user's password is sent as it is to the server for authentication. This is an important security measure which every developer should be aware of. So in this article, we will discuss how we can encrypt user password ono frontend using javascipt and how we will decrypt it in our python backend and use that for user authentication. First of all, we need to download the aes.js file. You can download the file with the link: aes.js Next thing we are going to do is we will add the downloaded file to our project's static folder and then import in our base.html file. I assume that this is already setup in yourr codebase. Now we will stop the login form submission using javascript/jQuery and first encrypt the password! Below is the javascript code we are going to be adding: $ ( '#login-click' ). on ( 'click' , function ( e ){ e . preventDefault (); var passwd = $ ( '#password_field' ). val (); var key = CryptoJS . enc . Utf8 . parse ( '_demo_application' ); var iv = CryptoJS . enc . Utf8 . parse ( '1234567812345678' ); var encryptPass = CryptoJS . AES . encrypt ( passwd , key , { iv : iv , mode : CryptoJS . mode . CBC , padding : CryptoJS . pad . Pkcs7 }); $ ( '#password_field' ). val ( encryptPass ); $ ( \".login-form\" ). submit (); }); Okay, so now we are sending the user password in encrypted form using CryptoJS. Next thing we need to achieve is to accept the username and password in our backend code, and decrypt the password to get the real password and authenticate with oour user table. As we move to the backend python code, Lets make sure these two libraries are imported in your py file: import base64 from Crypto.Cipher import AES To be able to import AES from Crypto.Cipher, we need to first install some libraries: pip3 install pycrypto pip3 install pycryptodome Once we install the above libraries, our imports will start working. Now we are going to do the decyption of encrypted password. The python code is below: @auth . route ( '/login' , methods = [ 'GET' , 'POST' ]) def login (): if request . method == 'POST' : username = request . form . get ( 'username' ) password = request . form . get ( 'password' ) # ... Your code # ... Your code in_key = '_demo_application' in_iv = '1234567812345678' x = AES . new ( in_key . encode ( \"utf-8\" ), AES . MODE_CBC , in_iv . encode ( \"utf-8\" )) cipher_text = base64 . decodebytes (( password ) . encode ( \"utf-8\" )) padded_pass = x . decrypt ( cipher_text ) . decode ( \"utf-8\" ) unpad_pkcs5 = lambda x : x [: - ord ( x [ - 1 ])] password = unpad_pkcs5 ( padded_pass ) # ... Now we have actual decrypted password of the user and we can carry on with authentication # ... login_user(username, password) As we can see in code above, we have derived the decypted password and we can use that for authentication using flask's login methods. We can be very creative with this, we can send the KEY and IV values randomly generated from our backend which we will store in the session and use at the time of decryption. This way KEY and IV will be different for every login attempt. That is all, I hope this was helpful to you and solved your problem. Also note that CBC mode of encryption is vulnerable against LUCKY13 attack (i don't know what it is.. must read it later sometime). so I suggest you to change the mode of cipher if you really want top notch security. Anyway this was enough to solve my problems, Now if you find this less detailed, I encourage you to watch the video that I have created, where I do all the imports and show the running version live. Thank you for taking time to read this article. I will see you in the next one!!! Please find the video demonstration youtube video link: DISCLAIMER: The author doesn't intend to be expert on the problems. While these notes will solve your problem at hand, it is always advised to dig deep to the problem if one is interested and also share your further learnings in the comments below as well.","tags":"programming","url":"https://prakharmishra.com/articles/aes-encrypt-user-password-over-http.html","loc":"https://prakharmishra.com/articles/aes-encrypt-user-password-over-http.html"},{"title":"Fixing Host Header Injection in Flask/Nginx setup","text":"Host header injection is a very common vulnerability found in today's websites. An adversary could use this attack to poison the host header and further carry out malicious activities on the victim. In Http request-response model, host header is one of the header found in request made by client. Sometimes web servers use this header information to redirect the request to appropriate application running on some port in the server, sometimes web application use this header to create absolute urls added in thier rounting file (such as urls.py file in Django). The issue is, since the host header is sent by client, we can never trust this data to make server side decisions. Two of the most severe attacks an adversary can do are: Password reset poisoning. Web cache poisoning. Other kind of security breaches can also be done on top of this vulnerablity. For an example, an attacker can get the secret token to reset a user's password by passing malicious header. The web application will then create an absolute URL with malicious header which has the secret token and send the password reset link to the real user's mail. If user happens to click on that reset link which is recieved in his/her mail out of nowhere, User will pass that secret token to an attacker controlled host. To mitigate this, there are two kinds of check which we can apply: Whitelist hosts in our web application. Whitelist hosts in our web server conf. (Our nginx server). Some frameworks such as Django provide configuration variable in settings file while in other framworks we have to check the host header before we process any request. In Django, the setting variable is ALLOWED_HOSTS = [] , where we have to provide host names as a list. But more than often, people put an astrix '*' in that list and carry on this to the production. I advice you to put your IP address or domain name or both in that list and then go for production. This will fix the issue at application level and you will be free of this vulnerability. In Flask, We have a decorator named @app.before_request which allows us to manipulate and validate any requests coming to the server. @app . before_request def check_for_maintenance (): if request . host not in [ '10.x.y.z' , 'www.mywebsite.com' ]: return redirect ( url_for ( '_site/403.html' )) if IS_MAINTENANCE_MODE and request . path != url_for ( \"maintenance\" ): return redirect ( url_for ( 'maintenance' )) This is how we handle and fix the issue using flask/django. Now let's discuss how we can block maliciouos headers using our nginx: Open your nginx.conf file (in my case path is /usr/local/conf/nginx.conf ) Add the below code in your server block: server { listen 443 ssl ; ... ... # Deny bad headers if ( $host !~* &#94; ( www.mywebsite.com ) $ ) { return 444 ; } } Voila, you are good to go. Now nginx is not gonna accept any header other than your mentioned whitelisted one. I hope you found this blog helpful, write in the comments if you have any doubts or if you get stuck somewhere. Thanks for reading this, and I will see you in the next one! DISCLAIMER: The author doesn't intend to be expert on the problems. While these notes will solve your problem at hand, it is always advised to dig deep to the problem if one is interested and also share your further learnings in the comments below as well.","tags":"programming","url":"https://prakharmishra.com/articles/host-header-injection-1.html","loc":"https://prakharmishra.com/articles/host-header-injection-1.html"},{"title":"My First Post","text":"This is the first post . YAY!","tags":"misc","url":"https://prakharmishra.com/articles/My-first-post.html","loc":"https://prakharmishra.com/articles/My-first-post.html"}]};